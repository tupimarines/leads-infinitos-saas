# Google Maps Scraper  

A lightweight, customizable web scraper built with **Playwright** to extract business listings from Google Maps. Perfect for gathering contact details, addresses, ratings, and more.  

**Note:** This project is for **educational purposes only**. Always respect Google's Terms of Service and scraping policies.  

---

## üìÇ Output Examples  
The data is saved in a folder named `GMaps Data` in a folder of the date the script was executed.
Check the generated files to understand the data structure:  
- **`niche in place.csv`**  
- **`niche in place.xlsx`**  
As you specified in either the command or the txt file (niche and place).

Each entry includes:  
- Business name  
- Rating (avg. and count)  
- Contact info (phone, website)  
- Address & location details  
- Additional metadata (reviews, features, etc.)  

---

## ‚öôÔ∏è Installation  

### 1. Set Up a Virtual Environment (Recommended)  
```bash
virtualenv venv  
source venv/bin/activate  # Linux/Mac  
venv\Scripts\activate     # Windows  
```  

### 2. Install Dependencies  
```bash
pip install -r requirements.txt  
playwright install chromium  # Headless browser for scraping  
```  

---

## üöÄ How to Run  

### Option 0: Web App (com autentica√ß√£o)  
1. Defina vari√°veis (opcional):
   - `FLASK_SECRET_KEY` (recomendado em prod)
   - `STORAGE_DIR` (ex.: `storage` ou `/data/storage` em prod)
2. Inicie o app:
   ```bash
   python app.py
   ```  
3. Acesse `http://localhost:8000` ‚Üí primeiro registre-se, depois fa√ßa login e use a tela de extra√ß√£o.

Os arquivos gerados s√£o salvos em `storage/<userId>/<YYYY-MM-DD>/...` e os downloads s√£o protegidos por usu√°rio.

### Deploy com Dokploy (Dockerfile)
1. Fa√ßa push deste reposit√≥rio.
2. No Dokploy: New App ‚Üí Dockerfile ‚Üí selecione este repo.
3. Porta: 8000. Healthcheck: `/healthz`.
4. Vari√°veis de ambiente: `FLASK_SECRET_KEY`, `STORAGE_DIR=/app/storage`.
5. Volumes: monte volumes persistentes em `/app/app.db` (SQLite) e `/app/storage` (arquivos por usu√°rio).
6. Opcional: ajuste workers Gunicorn via `WEB_CONCURRENCY`.

---

## üõ†Ô∏è Admin scripts (Python)

### Criar/atualizar usu√°rio (`create_user.py`)

Criar usu√°rio simples:
```bash
python create_user.py --email "cliente@exemplo.com" --password "senha123456"
```

Criar usu√°rio e licen√ßa anual:
```bash
python create_user.py --email "cliente@exemplo.com" --password "senha123456" --create-license --license-type anual
```

Criar usu√°rio e licen√ßa "vital√≠cia" (anual com expira√ß√£o de 50 anos):
```bash
python create_user.py --email "cliente@exemplo.com" --password "senha123456" --create-license --lifetime
```

Atualizar senha de usu√°rio existente:
```bash
python create_user.py --email "cliente@exemplo.com" --password "novaSenha" --update-password
```

### Criar licen√ßas anuais para todos os usu√°rios (`create_annual_licenses.py`)

Apenas para quem ainda n√£o tem nenhuma licen√ßa:
```bash
python create_annual_licenses.py --yes
```

For√ßar cria√ß√£o para todos (mesmo que j√° tenham licen√ßa):
```bash
python create_annual_licenses.py --yes --force
```

Personalizar dias at√© expira√ß√£o (padr√£o: 365):
```bash
python create_annual_licenses.py --yes --expires-days 365
```

### Outros utilit√°rios

Inicializar banco (primeiro uso):
```bash
python init_db.py
```

Listar usu√°rios e licen√ßas:
```bash
python list_all_users.py
```

### Option 1: Single Search  
```bash
python3 main.py -s="<query>" -t=<result_count>  
```  
**Example:**  
```bash
python3 main.py -s="coffee shops in Seattle" -t=50  
```  

### Option 2: Batch Searches (via `input.txt`)  
1. Add queries to **`input.txt`** (one per line):  
   ```text
   dentists in Boston, MA  
   plumbers in Austin, TX  
   ```  
2. Run the scraper:  
   ```bash
   python3 main.py -t=30  # Optional: Limit results per query  
   ```  

---

## üí° Pro Tips  

### Maximizing Results  
Google Maps limits visible results (~120 per search). To bypass this:  
- **Use granular queries** (e.g., split "US dentists" into city/state-level searches).  
- **Combine keywords** (e.g., `"emergency dentist Chicago 24/7"`).  

### Customization  
- Adjust **`main.py`** to scrape additional fields (e.g., hours, pricing).  
- Modify **`playwright`** settings in `scraper.py` to change timeouts or headless mode.  

---

## ‚ùì Troubleshooting  
- **Slow scraping?** Add delays between requests (edit `scraper.py`).  
- **Missing data?** Google may block frequent requests‚Äîtry proxies or reduce speed.  

--- 

**Happy scraping!** üõ†Ô∏è